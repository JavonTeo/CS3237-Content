{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS3237 Lab 4 - Neural Networks and Deep Learning\n",
    "\n",
    "**Name: Javon Teo Tze Kai**\n",
    "\n",
    "**Student Number: A0233706J**\n",
    "\n",
    "**Lab Group: B02**\n",
    "\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "The objectives of this lab are:\n",
    "\n",
    "    1. To familiarize you with how to encode input and output vectors for neural networks.\n",
    "    2. To give you some insight into how hyperparameters like learning rate and momentum affect training.\n",
    "    3. To create, test and train, a CNN deep learning model using the MNIST dataset.\n",
    "    \n",
    "To save time we will train each experiment only for 10 epochs. This will lead to less than optimal results but is enough for you to make observations.\n",
    "\n",
    "## 2. The Irises Dataset\n",
    "\n",
    "We will now work again on the Irises Dataset, which we used in Lab 3, for classifying iris flowers into one of three possible types. As before we will consider four factors:\n",
    "\n",
    "    1. Sepal length in cm\n",
    "    2. Sepal width in cm\n",
    "    3. Petal length in cm\n",
    "    4. Petal width in cm\n",
    "\n",
    "In this dataset there are 150 sample points. The code below loads the dataset and prints the first 10 rows so we have an idea of what it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 rows of data:\n",
      "[[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]\n",
      " [5.4 3.9 1.7 0.4]\n",
      " [4.6 3.4 1.4 0.3]\n",
      " [5.  3.4 1.5 0.2]\n",
      " [4.4 2.9 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "iris = load_iris()\n",
    "\n",
    "print(\"First 10 rows of data:\")\n",
    "print(iris.data[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Scaling the Data\n",
    "\n",
    "We make use of the MinMaxScaler to scale the inputs to between 0 and 1.  The code below does this and prints the first 10 rows again, to show us the difference.\n",
    "\n",
    "In the next section we will investigate what happens if we use unscaled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 rows of SCALED data.\n",
      "[[0.22222222 0.625      0.06779661 0.04166667]\n",
      " [0.16666667 0.41666667 0.06779661 0.04166667]\n",
      " [0.11111111 0.5        0.05084746 0.04166667]\n",
      " [0.08333333 0.45833333 0.08474576 0.04166667]\n",
      " [0.19444444 0.66666667 0.06779661 0.04166667]\n",
      " [0.30555556 0.79166667 0.11864407 0.125     ]\n",
      " [0.08333333 0.58333333 0.06779661 0.08333333]\n",
      " [0.19444444 0.58333333 0.08474576 0.04166667]\n",
      " [0.02777778 0.375      0.06779661 0.04166667]\n",
      " [0.16666667 0.45833333 0.08474576 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "scaler = MinMaxScaler()\n",
    "scaler.fit(iris.data)\n",
    "X = scaler.transform(iris.data)\n",
    "Y = iris.target\n",
    "\n",
    "print(\"First 10 rows of SCALED data.\")\n",
    "print(X[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Encoding the Targets\n",
    "\n",
    "In Lab 3 we saw that the target values (type of iris flower) is a vector from 0 to 2. We can see the 150 labels below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n"
     ]
    }
   ],
   "source": [
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this to train the neural network, but we will use \"one-hot\" encoding, where we have a vector of _n_ integers consisting of 0's and 1's.  The table below shows how one-hot encoding works:\n",
    "\n",
    "|   Value    |    One-Hot Encoding    |\n",
    "|:----------:|:----------------------:|\n",
    "| 0 | \\[1 0 0\\] |\n",
    "| 1 | \\[0 1 0\\] |\n",
    "| 2 | \\[0 0 1\\] |\n",
    "\n",
    "Pytorch provides the one_hot function to create one-hot vectors:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "print(F.one_hot(torch.tensor(iris.target).to(torch.int64), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's split the data into training and testing data:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "train_x, test_x, train_y, test_y = train_test_split(X, Y, test_size = 0.2, random_state = 1)\n",
    "train_x = torch.Tensor(train_x)\n",
    "train_y = F.one_hot(torch.tensor(train_y).to(torch.int64), 3).to(torch.float32)\n",
    "\n",
    "test_x = torch.Tensor(test_x)\n",
    "test_y = F.one_hot(torch.tensor(test_y).to(torch.int64), 3).to(torch.float32)\n",
    "\n",
    "train_dataset = TensorDataset(train_x, train_y)\n",
    "test_dataset = TensorDataset(test_x, test_y)\n",
    "\n",
    "train_loader = DataLoader(train_dataset)\n",
    "test_loader = DataLoader(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Building our Neural Network\n",
    "\n",
    "Let's now begin building a simple neural network with a single hidden layer, using the Stochastic Gradient Descent (SGD) optimizer, ReLu transfer functions for the hidden layer and softmax for the output layer.\n",
    "\n",
    "The code to do this is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class ModelNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ModelNN, self).__init__()\n",
    "        self.l1 = nn.Linear(4, 100)\n",
    "        self.l2 = nn.Linear(100, 3)\n",
    "    def forward(self, x):\n",
    "        x = self.l1(x)\n",
    "        x = F.relu(x)\n",
    "        output = self.l2(x)\n",
    "        return output\n",
    "\n",
    "model = ModelNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.1, momentum = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Training the Neural Network\n",
    "\n",
    "As is usually the case, we can call the \"fit\" method to train the neural network for 10 epochs. You can increase this to a larger value if you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/100], Loss: 0.0076\n",
      "Epoch [10/100], Loss: 0.0019\n",
      "Epoch [15/100], Loss: 0.0015\n",
      "Epoch [20/100], Loss: 0.0009\n",
      "Epoch [25/100], Loss: 0.0006\n",
      "Epoch [30/100], Loss: 0.0004\n",
      "Epoch [35/100], Loss: 0.0003\n",
      "Epoch [40/100], Loss: 0.0003\n",
      "Epoch [45/100], Loss: 0.0003\n",
      "Epoch [50/100], Loss: 0.0002\n",
      "Epoch [55/100], Loss: 0.0002\n",
      "Epoch [60/100], Loss: 0.0002\n",
      "Epoch [65/100], Loss: 0.0002\n",
      "Epoch [70/100], Loss: 0.0001\n",
      "Epoch [75/100], Loss: 0.0001\n",
      "Epoch [80/100], Loss: 0.0001\n",
      "Epoch [85/100], Loss: 0.0001\n",
      "Epoch [90/100], Loss: 0.0001\n",
      "Epoch [95/100], Loss: 0.0001\n",
      "Epoch [100/100], Loss: 0.0001\n",
      "Training complete!\n",
      "Training accuracy of the model: 95.0 %\n",
      "Test accuracy of the model: 100.0 %\n"
     ]
    }
   ],
   "source": [
    "# Train the Model\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (data, labels) in enumerate(train_loader):\n",
    "        # Forward pass\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, labels)\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "print(\"Training complete!\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for input, labels in train_loader:\n",
    "        outputs = model(input)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        _, label = torch.max(labels, 1)\n",
    "        total += input.size(dim = 0)\n",
    "        correct += (predicted == label).sum().item()\n",
    "    print(f'Training accuracy of the model: {100 * correct / total} %')\n",
    "\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    i = 0\n",
    "    for input, labels in test_loader:\n",
    "        outputs = model(input)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        _, label = torch.max(labels, 1)\n",
    "        total += input.size(dim = 0)\n",
    "        correct += (predicted == label).sum().item()\n",
    "    print(f'Test accuracy of the model: {100 * correct / total} %')"
   ]
  },
  {
   "attachments": {
    "83f3b67f-5c0e-4001-8e68-0c36aa71d8d0.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Question 1\n",
    "\n",
    "Run the code above. Do you see evidence of underfitting? Overfitting? Justify your answers. ***(4 MARKS)***\n",
    "\n",
    "**Answer: There is no evidence of underfitting or overfitting, because the training (93.3%) and test accuracy (100.0%) are about the same, and they also quite accurate in predicting the model. There is no evidence of overfitting, because the test accuracy is larger than the training accuracy.**\n",
    "\n",
    "_(For TA) Marks awarded: ____ / 4_\n",
    "\n",
    "---\n",
    "\n",
    "#### Question 2a\n",
    "\n",
    "Consult the documentation for the SGD optimizer [here](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html). What does the lr parameter do? ***(1 MARK)***\n",
    "\n",
    "**Answer: The lr parameter influences how quickly the weights of the model are adjusted at each step.**\n",
    "\n",
    "#### Question 2b\n",
    "\n",
    "The momentum parameter \"accelerates gradient descent in the relevant direction and dampens oscillations\". Using Google or other means, illustrate what this means. ***(2 MARKS)***\n",
    "\n",
    "**Answer: The momentum parameter smoothens the gradient descent process by accumulating past gradients, helping gradient descent to move consistently in the same direction. From the image found by Google:\n",
    "![image.png](attachment:83f3b67f-5c0e-4001-8e68-0c36aa71d8d0.png), when the momentum parameter is 0 (blue line), the gradient descent is not smooth, while when there is momentum (yellow line), the gradient descent is smoother.**\n",
    "\n",
    "_(For TA) Marks awarded: ____ / 3_\n",
    "\n",
    "----\n",
    "\n",
    "#### Question 3a\n",
    "\n",
    "We will now play with the lr parameter. Adjust the lr parameter to the following values and record the final training and test accuracies in the respective columns. Also observe the sequence of accuracies over the training period, and place your observation in the \"remarks\" column, e.g. \"Progresses steadily\", \"some oscillation\" etc. ***(3 MARKS)***\n",
    "\n",
    "**Answer: Fill the table below **\n",
    "\n",
    "|  lr    | Training Acc. | Testing    Acc. |      Remarks      |\n",
    "|:------:|---------------|-----------------|-------------------|\n",
    "|0.01    |    93.33%     |    93.33%       |Progresses steadily, loss decreases steadily starting from 0.1780.|\n",
    "|0.1     |    88.33%     |    93.33%       |Progresses steadily, loss decreases steadily starting from 0.0072.|\n",
    "|1.0     |    69.17%     |    56.67%       |Model has low loss but low accuracy, the loss oscillates around 0.|\n",
    "|10.0    |    32.50%     |    36.67%       |Loss oscillates around 9-10.|\n",
    "|100     |    32.50%     |    36.67%       |Loss oscillates around 108.|\n",
    "|1000    |    32.50%     |    36.67%       |Loss oscillates around 1017.|\n",
    "|10000   |    32.50%     |    36.67%       |Loss oscillates around 8532.|\n",
    "|100000  |    32.50%     |    36.67%       |Loss oscillates around 104392.|\n",
    "\n",
    "\n",
    "#### Question 3b\n",
    "\n",
    "Based on your observations above, comment on the effect of small and very large learning rates on the learning. ***(2 MARKS)***\n",
    "\n",
    "**Answer: When the learning rate is small and not optimal, it leads to slow convergence and this may give a bad training and testing accuracy if the number of epochs is small. When the learning rate is very large, it may overshoot or oscillate around the optimal solution and hence also give bad training and testing accuracy.**\n",
    "\n",
    "_(For TA) Marks awarded: ____ / 5_\n",
    "\n",
    "### 2.5 Using Momentum\n",
    "\n",
    "We will now experiment with the momentum term. To do this:\n",
    "\n",
    "    1. Change the learning rate to 0.1.\n",
    "    2. Set the momentum to 0.1. \n",
    "    \n",
    "Run your neural network.\n",
    "\n",
    "---\n",
    "\n",
    "#### Question 4a\n",
    "\n",
    "Keeping the learning rate at 0.1, complete the table below using the momentum values shown. Again record any observations in the \"Remarks\" column. ***(3 MARKS)***\n",
    "\n",
    "**Answer: Fill the table below**\n",
    "\n",
    "| momentum | Training Acc. | Testing    Acc. |      Remarks      |\n",
    "|:--------:|---------------|-----------------|-------------------|\n",
    "|0.001     |     89.17%    |     93.33%      |Progresses steadily|\n",
    "|0.01      |     88.33%    |     93.33%      |Progresses steadily|\n",
    "|0.1       |     85.83%    |     90.00%      |Progresses steadily|\n",
    "|1.0       |     30.83%    |     43.33%      |Much Oscillation|\n",
    "\n",
    "#### Question 4b\n",
    "\n",
    "Based on your observations above, does the momentum term help in learning? ***(2 MARKS)***\n",
    "\n",
    "**Answer: The momentum term may help in learning, but this depends on its value. If it is too high, it may overshoot the optimal solution.**\n",
    "\n",
    "_(For TA) Marks awarded: ____ / 5_\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## 4. Creating a CNN for the MNIST Dataset\n",
    "\n",
    "In this section we will now create a convolutional neural network (CNN) to classify images in the MNIST dataset that we used in Lecture 5. Let's build each part step by step.\n",
    "\n",
    "### 4.1 Loading the MNIST Dataset\n",
    "\n",
    "As in the Neural Network example from Lecture 5 we will load the MNIST dataset, scale the inputs to between 0 and 1, and convert the Y labels to one-hot vectors. However unlike before we will not flatten the 28x28 image to a 784 element vector, since CNNs can inherently handle 2D data. We will use a batch size of 1 for now, you may want to try other values for your training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "# Load the data, and normalise it.\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5,), (0.5,))])\n",
    "# Change target to a one hot vector.\n",
    "target_transform = transforms.Compose(\n",
    "    [transforms.Lambda(lambda x: F.one_hot(torch.tensor(x), 10))])\n",
    "\n",
    "training_set = datasets.MNIST('data', train=True, download=True, transform=transform, target_transform=target_transform)\n",
    "test_set = datasets.MNIST('data', train=False, transform=transform, target_transform=target_transform)\n",
    "train_loader = torch.utils.data.DataLoader(training_set, batch_size = batch_size)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Create your CNN network\n",
    "\n",
    "Create your CNN network using pytorch (Hint: you may look at the CNN example from lecture 6). You should minimally have two convolutional layers, two maxpool layers, and at least one dense (linear) layer for the output. Use activation functions between layers such as `relu` or `softmax`. Pay careful attention to the size of the inputs and outputs. \n",
    "\n",
    "Write your class in the cell below and implement both the `constructor` and the `forward` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Enter your code for part 4.2 here in this code cell.\n",
    "\"\"\"\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        # call the parent constructor\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels = 1, out_channels = 32, kernel_size = (5,5))\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size = (2,2), stride = (2,2))\n",
    "        self.conv2 = nn.Conv2d(in_channels = 32, out_channels = 64, kernel_size = (5,5))\n",
    "        self.fc1 = nn.Linear(in_features=64*4*4, out_features=10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        output = self.fc1(x)\n",
    "        return output\n",
    "\n",
    "model = CNNModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Train your CNN network\n",
    "\n",
    "Now train your network on the training dataset. You should train for 50 to 100 epochs.\n",
    "\n",
    "You will have to decide on which optimizer to use (SGD, adam, for example), which loss function you will use, and any other hyperparameters such as learning rate, batch size, and momentum.\n",
    "\n",
    "Write your code to train in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Training Loss: 23315.81640625\n",
      "Epoch 1 Training Loss: 6754.41162109375\n",
      "Epoch 2 Training Loss: 4841.7705078125\n",
      "Epoch 3 Training Loss: 3952.482421875\n",
      "Epoch 4 Training Loss: 3408.28125\n",
      "Epoch 5 Training Loss: 3028.119384765625\n",
      "Epoch 6 Training Loss: 2741.784912109375\n",
      "Epoch 7 Training Loss: 2512.85986328125\n",
      "Epoch 8 Training Loss: 2322.572998046875\n",
      "Epoch 9 Training Loss: 2161.57470703125\n",
      "Epoch 10 Training Loss: 2023.096435546875\n",
      "Epoch 11 Training Loss: 1900.8817138671875\n",
      "Epoch 12 Training Loss: 1792.36767578125\n",
      "Epoch 13 Training Loss: 1694.1556396484375\n",
      "Epoch 14 Training Loss: 1606.1148681640625\n",
      "Epoch 15 Training Loss: 1525.03564453125\n",
      "Epoch 16 Training Loss: 1449.6240234375\n",
      "Epoch 17 Training Loss: 1379.112548828125\n",
      "Epoch 18 Training Loss: 1313.6612548828125\n",
      "Epoch 19 Training Loss: 1252.3280029296875\n",
      "Epoch 20 Training Loss: 1194.19140625\n",
      "Epoch 21 Training Loss: 1140.1312255859375\n",
      "Epoch 22 Training Loss: 1088.5277099609375\n",
      "Epoch 23 Training Loss: 1039.49267578125\n",
      "Epoch 24 Training Loss: 993.2709350585938\n",
      "Epoch 25 Training Loss: 949.5225830078125\n",
      "Epoch 26 Training Loss: 907.7392578125\n",
      "Epoch 27 Training Loss: 868.4315185546875\n",
      "Epoch 28 Training Loss: 831.3409423828125\n",
      "Epoch 29 Training Loss: 795.3457641601562\n",
      "Epoch 30 Training Loss: 761.1445922851562\n",
      "Epoch 31 Training Loss: 728.914306640625\n",
      "Epoch 32 Training Loss: 697.9529418945312\n",
      "Epoch 33 Training Loss: 668.6688232421875\n",
      "Epoch 34 Training Loss: 640.16845703125\n",
      "Epoch 35 Training Loss: 613.1521606445312\n",
      "Epoch 36 Training Loss: 586.6959228515625\n",
      "Epoch 37 Training Loss: 561.8428955078125\n",
      "Epoch 38 Training Loss: 537.5096435546875\n",
      "Epoch 39 Training Loss: 514.42333984375\n",
      "Epoch 40 Training Loss: 492.18951416015625\n",
      "Epoch 41 Training Loss: 471.0248107910156\n",
      "Epoch 42 Training Loss: 450.52459716796875\n",
      "Epoch 43 Training Loss: 430.4714660644531\n",
      "Epoch 44 Training Loss: 411.91064453125\n",
      "Epoch 45 Training Loss: 393.42138671875\n",
      "Epoch 46 Training Loss: 376.8248291015625\n",
      "Epoch 47 Training Loss: 360.27642822265625\n",
      "Epoch 48 Training Loss: 344.8316955566406\n",
      "Epoch 49 Training Loss: 329.9004821777344\n",
      "Epoch 50 Training Loss: 316.0349426269531\n",
      "Epoch 51 Training Loss: 302.4588928222656\n",
      "Epoch 52 Training Loss: 289.47369384765625\n",
      "Epoch 53 Training Loss: 277.59771728515625\n",
      "Epoch 54 Training Loss: 265.69488525390625\n",
      "Epoch 55 Training Loss: 255.15469360351562\n",
      "Epoch 56 Training Loss: 244.9934539794922\n",
      "Epoch 57 Training Loss: 235.45843505859375\n",
      "Epoch 58 Training Loss: 225.97061157226562\n",
      "Epoch 59 Training Loss: 216.5446014404297\n",
      "Epoch 60 Training Loss: 207.9528045654297\n",
      "Epoch 61 Training Loss: 199.81350708007812\n",
      "Epoch 62 Training Loss: 191.9042205810547\n",
      "Epoch 63 Training Loss: 184.30776977539062\n",
      "Epoch 64 Training Loss: 176.98829650878906\n",
      "Epoch 65 Training Loss: 169.66505432128906\n",
      "Epoch 66 Training Loss: 162.66908264160156\n",
      "Epoch 67 Training Loss: 155.49009704589844\n",
      "Epoch 68 Training Loss: 148.77174377441406\n",
      "Epoch 69 Training Loss: 141.90733337402344\n",
      "Epoch 70 Training Loss: 135.63865661621094\n",
      "Epoch 71 Training Loss: 129.66461181640625\n",
      "Epoch 72 Training Loss: 123.62313842773438\n",
      "Epoch 73 Training Loss: 118.29646301269531\n",
      "Epoch 74 Training Loss: 112.9836654663086\n",
      "Epoch 75 Training Loss: 108.47779846191406\n",
      "Epoch 76 Training Loss: 103.38114166259766\n",
      "Epoch 77 Training Loss: 99.3802719116211\n",
      "Epoch 78 Training Loss: 95.4636459350586\n",
      "Epoch 79 Training Loss: 91.47921752929688\n",
      "Epoch 80 Training Loss: 87.63055419921875\n",
      "Epoch 81 Training Loss: 83.69839477539062\n",
      "Epoch 82 Training Loss: 80.332275390625\n",
      "Epoch 83 Training Loss: 77.27216339111328\n",
      "Epoch 84 Training Loss: 74.09497833251953\n",
      "Epoch 85 Training Loss: 71.25747680664062\n",
      "Epoch 86 Training Loss: 68.15696716308594\n",
      "Epoch 87 Training Loss: 65.34346771240234\n",
      "Epoch 88 Training Loss: 62.1685905456543\n",
      "Epoch 89 Training Loss: 59.22982406616211\n",
      "Epoch 90 Training Loss: 56.0914421081543\n",
      "Epoch 91 Training Loss: 54.04399871826172\n",
      "Epoch 92 Training Loss: 51.79032897949219\n",
      "Epoch 93 Training Loss: 49.21458435058594\n",
      "Epoch 94 Training Loss: 46.971927642822266\n",
      "Epoch 95 Training Loss: 44.708370208740234\n",
      "Epoch 96 Training Loss: 43.29758071899414\n",
      "Epoch 97 Training Loss: 41.033775329589844\n",
      "Epoch 98 Training Loss: 40.02140426635742\n",
      "Epoch 99 Training Loss: 38.087547302246094\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Enter your code for part 4.3 here in this code cell.\n",
    "\"\"\"\n",
    "# define training hyperparameters\n",
    "lr = 1e-5\n",
    "num_epochs = 100\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "for e in range(0, num_epochs):\n",
    "\t# set the model in training mode\n",
    "\tmodel.train()\n",
    "\t# initialize the total training and validation loss\n",
    "\ttotalTrainLoss = 0\n",
    "\ttotalValLoss = 0\n",
    "\t# initialize the number of correct predictions in the training\n",
    "\t# and validation step\n",
    "\ttrainCorrect = 0\n",
    "\tvalCorrect = 0\n",
    "\t# loop over the training set\n",
    "\tfor i, (x, y) in enumerate(train_loader):\n",
    "\t\t# send the input to the device\n",
    "\t\t(x, y) = (x.to(device), y.to(torch.float32).to(device))\n",
    "\t\t# perform a forward pass and calculate the training loss\n",
    "\t\tpred = model(x)\n",
    "\t\tloss = criterion(pred, y)\n",
    "\t\t# zero out the gradients, perform the backpropagation step,\n",
    "\t\t# and update the weights\n",
    "\t\toptimizer.zero_grad()\n",
    "\t\tloss.backward()\n",
    "\t\toptimizer.step()\n",
    "\t\t# add the loss to the total training loss so far and\n",
    "\t\t# calculate the number of correct predictions\n",
    "\t\ttotalTrainLoss += loss\n",
    "\tprint(\"Epoch\", e, \"Training Loss:\", totalTrainLoss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Test your CNN network\n",
    "\n",
    "Now test your network on the test dataset. Print out the accuracy of your model. \n",
    "\n",
    "Try modifying your model and choosing different hyperparameters and see if you can improve the accuracy of your model. \n",
    "\n",
    "Write your code to test in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.004686\n",
      "\n",
      "Train Accuracy of     0: 100% (5923/5923)\n",
      "Train Accuracy of     1: 99% (6740/6742)\n",
      "Train Accuracy of     2: 99% (5952/5958)\n",
      "Train Accuracy of     3: 99% (6100/6131)\n",
      "Train Accuracy of     4: 99% (5841/5842)\n",
      "Train Accuracy of     5: 100% (5421/5421)\n",
      "Train Accuracy of     6: 99% (5914/5918)\n",
      "Train Accuracy of     7: 99% (6235/6265)\n",
      "Train Accuracy of     8: 99% (5842/5851)\n",
      "Train Accuracy of     9: 99% (5933/5949)\n",
      "\n",
      "Train Accuracy (Overall): 99% (59901/60000)\n",
      "\n",
      "Test Loss: 0.067711\n",
      "\n",
      "Test Accuracy of     0: 99% (979/980)\n",
      "Test Accuracy of     1: 100% (1135/1135)\n",
      "Test Accuracy of     2: 99% (1022/1032)\n",
      "Test Accuracy of     3: 98% (996/1010)\n",
      "Test Accuracy of     4: 99% (978/982)\n",
      "Test Accuracy of     5: 99% (884/892)\n",
      "Test Accuracy of     6: 98% (943/958)\n",
      "Test Accuracy of     7: 98% (1012/1028)\n",
      "Test Accuracy of     8: 98% (961/974)\n",
      "Test Accuracy of     9: 98% (990/1009)\n",
      "\n",
      "Test Accuracy (Overall): 99% (9900/10000)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Enter your code for part 4.4 here in this code cell.\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "classes = ('0', '1', '2', '3', '4', '5', '6', '7', '8', '9')\n",
    "\n",
    "#Get Training Accuracy\n",
    "#track train loss\n",
    "train_loss = 0.0\n",
    "class_correct = list(0. for i in range(10))\n",
    "class_total = list(0. for i in range(10))\n",
    "\n",
    "model.eval()\n",
    "# iterate over test data\n",
    "for i, (x, y) in enumerate(train_loader):\n",
    "\t# send the input to the device\n",
    "\t(x, y) = (x.to(device), y.to(torch.float32).to(device))\n",
    "\toutput = model(x)\n",
    "\t# calculate the batch loss\n",
    "\tloss = criterion(output, y)\n",
    "\t# update train loss \n",
    "\ttrain_loss += loss.item() * x.size(0)\n",
    "\t# convert output probabilities to predicted class\n",
    "\t_, pred = torch.max(output, 1)\n",
    "\t_, y = torch.max(y, 1)\n",
    "\t# compare predictions to true label\n",
    "\tcorrect_tensor = pred.eq(y)\n",
    "\tcorrect = np.squeeze(correct_tensor)\n",
    "    # calculate train accuracy for each object class\n",
    "\tfor i in range(len(y.data)):\n",
    "\t\tlabel = y.data[i]\n",
    "\t\tclass_correct[label] += correct.item() \n",
    "\t\tclass_total[label] += 1\n",
    "\n",
    "# average train loss\n",
    "train_loss = train_loss/len(train_loader.dataset)\n",
    "print('Train Loss: {:.6f}\\n'.format(train_loss))\n",
    "\n",
    "for i in range(10):\n",
    "    if class_total[i] > 0:\n",
    "        print('Train Accuracy of %5s: %2d%% (%2d/%2d)' % (\n",
    "            classes[i], 100.0 * class_correct[i] / class_total[i],\n",
    "            np.sum(class_correct[i]), np.sum(class_total[i])))\n",
    "    else:\n",
    "        print('Train Accuracy of %5s: N/A (no training examples)' % (classes[i]))\n",
    "\n",
    "print('\\nTrain Accuracy (Overall): %2d%% (%2d/%2d)\\n' % (\n",
    "    100. * np.sum(class_correct) / np.sum(class_total),\n",
    "    np.sum(class_correct), np.sum(class_total)))\n",
    "\n",
    "\n",
    "\n",
    "#Get Test Accuracy\n",
    "#track test loss\n",
    "test_loss = 0.0\n",
    "class_correct = list(0. for i in range(10))\n",
    "class_total = list(0. for i in range(10))\n",
    "\n",
    "model.eval()\n",
    "# iterate over test data\n",
    "for i, (x, y) in enumerate(test_loader):\n",
    "\t# send the input to the device\n",
    "\t(x, y) = (x.to(device), y.to(torch.float32).to(device))\n",
    "\toutput = model(x)\n",
    "\t# calculate the batch loss\n",
    "\tloss = criterion(output, y)\n",
    "\t# update test loss \n",
    "\ttest_loss += loss.item() * x.size(0)\n",
    "\t# convert output probabilities to predicted class\n",
    "\t_, pred = torch.max(output, 1)\n",
    "\t_, y = torch.max(y, 1)\n",
    "\t# compare predictions to true label\n",
    "\tcorrect_tensor = pred.eq(y)\n",
    "\tcorrect = np.squeeze(correct_tensor)\n",
    "    # calculate test accuracy for each object class\n",
    "\tfor i in range(len(y.data)):\n",
    "\t\tlabel = y.data[i]\n",
    "\t\tclass_correct[label] += correct.item() \n",
    "\t\tclass_total[label] += 1\n",
    "\n",
    "# average test loss\n",
    "test_loss = test_loss/len(test_loader.dataset)\n",
    "print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
    "\n",
    "for i in range(10):\n",
    "    if class_total[i] > 0:\n",
    "        print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (\n",
    "            classes[i], 100.0 * class_correct[i] / class_total[i],\n",
    "            np.sum(class_correct[i]), np.sum(class_total[i])))\n",
    "    else:\n",
    "        print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))\n",
    "\n",
    "print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (\n",
    "    100. * np.sum(class_correct) / np.sum(class_total),\n",
    "    np.sum(class_correct), np.sum(class_total)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 5:\n",
    "\n",
    "Complete the following table with your final design (you may add more rows for the # neurons (layer1) etc. to detail how many neurons you have in each hidden layer). Likewise you may replace the lr, momentum etc rows with parameters more appropriate to the optimizer that you have chosen. (4 MARKS)\n",
    "\n",
    "| Hyperparameter       | What I used | Why?                  |\n",
    "|:---------------------|:------------|:----------------------|\n",
    "| Optimizer            |Adam|Adam optimizer is able to adaptively adjusts the learning rate, allowing for optimum learning. It also includes a momentum-like term to smooth out the updates of weight. This helps the CNN learn faster and converge more quickly.|\n",
    "| Input shape          |(1, 1, 28, 28)|A small batch size of 1 is picked due to hardware and time constraints. Number of channels is set to 1 because the input images have only 1 channel as they are grayscale images. Each image is 28x28 image.|\n",
    "| First layer          |Convolution layer of 1 input channel, 32 output channels, (5, 5) kernel size|1 input channel due to greyscale input images. Based on research and experimentation, using 32 filters yields good results with sufficient feature representation, and without sacrificing significant computational costs. Kernel size of (5, 5) was chosen as it can capture complex local patterns and relationships within a moderately sized region.|\n",
    "| Second layer         |Convolution layer of 32 input channel, 64 output channels, (5, 5) kernel size|32 input channel due to previous convulational layer. The number of output channels was increased to 64 to allow the network to learn increasingly complex features and patterns. Kernel size of (5, 5) was unchanged from the previous layer.|\n",
    "| Pooling layers       |Max Pooling of kernel size (2,2) and stride (2,2). Used after each Convolution layer.|In MNIST, the digits are represented in white colour against a black background. This makes Max Pooling suitable for MNIST as it is able to extract the dominant brighter pixels from the image, making the model shift invariant and less noise sensitive. The kernel size (2, 2) and stride (2, 2) reduces the spatial dimensions of the feature maps to appropriate amounts.|\n",
    "| Dense layer          |Linear|Based on research and experiementation, a linear dense layer is suitable to learn the complex relationships between features in the data.|\n",
    "| learning rate?       |1e-5|A small learning rate of 1e-5 was used as the Adam optimizer is designed to handle adjusting the learning rate, and this learning rate is a good starting point for the optimizer.|\n",
    "| momentum?            |0|The momentum is set to 0. As I am using Adam optimizer, it already includes adaptive momentum, hence it would not be necessary to use it with an added momentum and might even lead to instability and worse performance.|\n",
    "| loss function?       |Cross-Entropy Loss|Cross-Entropy Loss function is commonly used for classification problems. It can calculate the difference between the predicated and true probabilites, and it has a smooth shape which makes it easier for gradient-based optimization methods to find the global minimum. Hence, it is suited for image classification of MNIST dataset.|\n",
    "\n",
    "*FOR GRADER:* <br>\n",
    "*TABLE: _____ / 4* <br>\n",
    "*CODE: ______ / 10*<br>\n",
    "\n",
    "***TOTAL: ______ / 14 ***\n",
    "\n",
    "#### Question 6\n",
    "\n",
    "What is the final training and test accuracy that you obtained after 100 epochs. Are there signs of underfitting or overfitting? Explain your answer (4 MARKS)\n",
    "\n",
    "**The final training and test accuracy is both 99%. There are no signs of underfitting, because both training and test accuracy are high, indicating that the model has learned the underlying patterns in the data effectively. There are also no signs of overfitting, because the test accuracy is not lower than the training accuracy, indicating that the model did not just memorize the training data and it can perform well on data it has not seen before.**\n",
    "\n",
    "*FOR GRADER: ______ / 4*\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Conclusion\n",
    "\n",
    "In this lab we saw how to create a simple Dense neural network to complete the relatively simple task of learning how to classify irises according to their sepal and petal characteristics. We then tried using a CNN on the MNIST dataset. \n",
    "\n",
    "---\n",
    "\n",
    "***FOR TA ONLY***\n",
    "\n",
    "| Question |  Marks  |\n",
    "|:--------:|:-------:|\n",
    "|1         |     /4  |\n",
    "|2         |     /3  |\n",
    "|3         |     /5  |\n",
    "|4         |     /5  |\n",
    "|5         |     /14 |\n",
    "|6         |     /4  |\n",
    "|Total:    |     /35 |\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "055b62fcaec9821674a26809055da6bc29fe87d96b4c426e8bdfbe57b9f21334"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
