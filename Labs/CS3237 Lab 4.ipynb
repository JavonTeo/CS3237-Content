{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS3237 Lab 4 - Neural Networks and Deep Learning\n",
    "\n",
    "**Name: Javon Teo Tze Kai**\n",
    "\n",
    "**Student Number: A0233706J**\n",
    "\n",
    "**Lab Group: B02**\n",
    "\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "The objectives of this lab are:\n",
    "\n",
    "    1. To familiarize you with how to encode input and output vectors for neural networks.\n",
    "    2. To give you some insight into how hyperparameters like learning rate and momentum affect training.\n",
    "    3. To create, test and train, a CNN deep learning model using the MNIST dataset.\n",
    "    \n",
    "To save time we will train each experiment only for 10 epochs. This will lead to less than optimal results but is enough for you to make observations.\n",
    "\n",
    "## 2. The Irises Dataset\n",
    "\n",
    "We will now work again on the Irises Dataset, which we used in Lab 3, for classifying iris flowers into one of three possible types. As before we will consider four factors:\n",
    "\n",
    "    1. Sepal length in cm\n",
    "    2. Sepal width in cm\n",
    "    3. Petal length in cm\n",
    "    4. Petal width in cm\n",
    "\n",
    "In this dataset there are 150 sample points. The code below loads the dataset and prints the first 10 rows so we have an idea of what it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 rows of data:\n",
      "[[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]\n",
      " [5.4 3.9 1.7 0.4]\n",
      " [4.6 3.4 1.4 0.3]\n",
      " [5.  3.4 1.5 0.2]\n",
      " [4.4 2.9 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "iris = load_iris()\n",
    "\n",
    "print(\"First 10 rows of data:\")\n",
    "print(iris.data[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Scaling the Data\n",
    "\n",
    "We make use of the MinMaxScaler to scale the inputs to between 0 and 1.  The code below does this and prints the first 10 rows again, to show us the difference.\n",
    "\n",
    "In the next section we will investigate what happens if we use unscaled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 rows of SCALED data.\n",
      "[[0.22222222 0.625      0.06779661 0.04166667]\n",
      " [0.16666667 0.41666667 0.06779661 0.04166667]\n",
      " [0.11111111 0.5        0.05084746 0.04166667]\n",
      " [0.08333333 0.45833333 0.08474576 0.04166667]\n",
      " [0.19444444 0.66666667 0.06779661 0.04166667]\n",
      " [0.30555556 0.79166667 0.11864407 0.125     ]\n",
      " [0.08333333 0.58333333 0.06779661 0.08333333]\n",
      " [0.19444444 0.58333333 0.08474576 0.04166667]\n",
      " [0.02777778 0.375      0.06779661 0.04166667]\n",
      " [0.16666667 0.45833333 0.08474576 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "scaler = MinMaxScaler()\n",
    "scaler.fit(iris.data)\n",
    "X = scaler.transform(iris.data)\n",
    "Y = iris.target\n",
    "\n",
    "print(\"First 10 rows of SCALED data.\")\n",
    "print(X[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Encoding the Targets\n",
    "\n",
    "In Lab 3 we saw that the target values (type of iris flower) is a vector from 0 to 2. We can see the 150 labels below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n"
     ]
    }
   ],
   "source": [
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this to train the neural network, but we will use \"one-hot\" encoding, where we have a vector of _n_ integers consisting of 0's and 1's.  The table below shows how one-hot encoding works:\n",
    "\n",
    "|   Value    |    One-Hot Encoding    |\n",
    "|:----------:|:----------------------:|\n",
    "| 0 | \\[1 0 0\\] |\n",
    "| 1 | \\[0 1 0\\] |\n",
    "| 2 | \\[0 0 1\\] |\n",
    "\n",
    "Pytorch provides the one_hot function to create one-hot vectors:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "print(F.one_hot(torch.tensor(iris.target), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's split the data into training and testing data:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "train_x, test_x, train_y, test_y = train_test_split(X, Y, test_size = 0.2, random_state = 1)\n",
    "train_x = torch.Tensor(train_x)\n",
    "train_y = F.one_hot(torch.tensor(train_y), 3).to(torch.float32)\n",
    "\n",
    "test_x = torch.Tensor(test_x)\n",
    "test_y = F.one_hot(torch.tensor(test_y), 3).to(torch.float32)\n",
    "\n",
    "train_dataset = TensorDataset(train_x, train_y)\n",
    "test_dataset = TensorDataset(test_x, test_y)\n",
    "\n",
    "train_loader = DataLoader(train_dataset)\n",
    "test_loader = DataLoader(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Building our Neural Network\n",
    "\n",
    "Let's now begin building a simple neural network with a single hidden layer, using the Stochastic Gradient Descent (SGD) optimizer, ReLu transfer functions for the hidden layer and softmax for the output layer.\n",
    "\n",
    "The code to do this is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class ModelNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ModelNN, self).__init__()\n",
    "        self.l1 = nn.Linear(4, 100)\n",
    "        self.l2 = nn.Linear(100, 3)\n",
    "    def forward(self, x):\n",
    "        x = self.l1(x)\n",
    "        x = F.relu(x)\n",
    "        output = self.l2(x)\n",
    "        return output\n",
    "\n",
    "model = ModelNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.1, momentum = 0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Training the Neural Network\n",
    "\n",
    "As is usually the case, we can call the \"fit\" method to train the neural network for 10 epochs. You can increase this to a larger value if you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10], Loss: 0.0071\n",
      "Epoch [10/10], Loss: 0.0025\n",
      "Training complete!\n",
      "Training accuracy of the model: 89.16666666666667 %\n",
      "Test accuracy of the model: 93.33333333333333 %\n"
     ]
    }
   ],
   "source": [
    "# Train the Model\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (data, labels) in enumerate(train_loader):\n",
    "        # Forward pass\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, labels)\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "print(\"Training complete!\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for input, labels in train_loader:\n",
    "        outputs = model(input)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        _, label = torch.max(labels, 1)\n",
    "        total += input.size(dim = 0)\n",
    "        correct += (predicted == label).sum().item()\n",
    "    print(f'Training accuracy of the model: {100 * correct / total} %')\n",
    "\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    i = 0\n",
    "    for input, labels in test_loader:\n",
    "        outputs = model(input)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        _, label = torch.max(labels, 1)\n",
    "        total += input.size(dim = 0)\n",
    "        correct += (predicted == label).sum().item()\n",
    "    print(f'Test accuracy of the model: {100 * correct / total} %')"
   ]
  },
  {
   "attachments": {
    "83f3b67f-5c0e-4001-8e68-0c36aa71d8d0.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Question 1\n",
    "\n",
    "Run the code above. Do you see evidence of underfitting? Overfitting? Justify your answers. ***(4 MARKS)***\n",
    "\n",
    "**Answer: There is no evidence of underfitting or overfitting, because the training (93.3%) and test accuracy (100.0%) are about the same, and they also quite accurate in predicting the model. There is no evidence of overfitting, because the test accuracy is larger than the training accuracy.**\n",
    "\n",
    "_(For TA) Marks awarded: ____ / 4_\n",
    "\n",
    "---\n",
    "\n",
    "#### Question 2a\n",
    "\n",
    "Consult the documentation for the SGD optimizer [here](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html). What does the lr parameter do? ***(1 MARK)***\n",
    "\n",
    "**Answer: The lr parameter influences how quickly the weights of the model are adjusted at each step.**\n",
    "\n",
    "#### Question 2b\n",
    "\n",
    "The momentum parameter \"accelerates gradient descent in the relevant direction and dampens oscillations\". Using Google or other means, illustrate what this means. ***(2 MARKS)***\n",
    "\n",
    "**Answer: The momentum parameter smoothens the gradient descent process by accumulating past gradients, helping gradient descent to move consistently in the same direction. From the image found by Google:\n",
    "![image.png](attachment:83f3b67f-5c0e-4001-8e68-0c36aa71d8d0.png), when the momentum parameter is 0 (blue line), the gradient descent is not smooth, while when there is momentum (yellow line), the gradient descent is smoother.**\n",
    "\n",
    "_(For TA) Marks awarded: ____ / 3_\n",
    "\n",
    "----\n",
    "\n",
    "#### Question 3a\n",
    "\n",
    "We will now play with the lr parameter. Adjust the lr parameter to the following values and record the final training and test accuracies in the respective columns. Also observe the sequence of accuracies over the training period, and place your observation in the \"remarks\" column, e.g. \"Progresses steadily\", \"some oscillation\" etc. ***(3 MARKS)***\n",
    "\n",
    "**Answer: Fill the table below **\n",
    "\n",
    "|  lr    | Training Acc. | Testing    Acc. |      Remarks      |\n",
    "|:------:|---------------|-----------------|-------------------|\n",
    "|0.01    |    93.33%     |    93.33%       |                   |\n",
    "|0.1     |    88.33%     |    93.33%       |                   |\n",
    "|1.0     |    69.17%     |    56.67%       |                   |\n",
    "|10.0    |    32.50%     |    36.67%       |                   |\n",
    "|100     |    32.50%     |    36.67%       |                   |\n",
    "|1000    |    32.50%     |    36.67%       |                   |\n",
    "|10000   |    32.50%     |    36.67%       |                   |\n",
    "|100000  |    32.50%     |    36.67%       |                   |\n",
    "\n",
    "\n",
    "#### Question 3b\n",
    "\n",
    "Based on your observations above, comment on the effect of small and very large learning rates on the learning. ***(2 MARKS)***\n",
    "\n",
    "**Answer: When the learning rate is small and not optimal, it leads to slow convergence and this may give a bad training and testing accuracy if the number of epochs is small. When the learning rate is very large, it may overshoot or oscillate around the optimal solution and hence also give bad training and testing accuracy.**\n",
    "\n",
    "_(For TA) Marks awarded: ____ / 5_\n",
    "\n",
    "### 2.5 Using Momentum\n",
    "\n",
    "We will now experiment with the momentum term. To do this:\n",
    "\n",
    "    1. Change the learning rate to 0.1.\n",
    "    2. Set the momentum to 0.1. \n",
    "    \n",
    "Run your neural network.\n",
    "\n",
    "---\n",
    "\n",
    "#### Question 4a\n",
    "\n",
    "Keeping the learning rate at 0.1, complete the table below using the momentum values shown. Again record any observations in the \"Remarks\" column. ***(3 MARKS)***\n",
    "\n",
    "**Answer: Fill the table below**\n",
    "\n",
    "| momentum | Training Acc. | Testing    Acc. |      Remarks      |\n",
    "|:--------:|---------------|-----------------|-------------------|\n",
    "|0.001     |     89.17%    |     93.33%      |                   |\n",
    "|0.01      |     88.33%    |     93.33%      |                   |\n",
    "|0.1       |     85.83%    |     90.00%      |                   |\n",
    "|1.0       |     30.83%    |     43.33%      |                   |\n",
    "\n",
    "#### Question 4b\n",
    "\n",
    "Based on your observations above, does the momentum term help in learning? ***(2 MARKS)***\n",
    "\n",
    "**Answer: The momentum term may help in learning, but this depends on its value. If it is too high, it may overshoot the optimal solution.**\n",
    "\n",
    "_(For TA) Marks awarded: ____ / 5_\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## 4. Creating a CNN for the MNIST Dataset\n",
    "\n",
    "In this section we will now create a convolutional neural network (CNN) to classify images in the MNIST dataset that we used in Lecture 5. Let's build each part step by step.\n",
    "\n",
    "### 4.1 Loading the MNIST Dataset\n",
    "\n",
    "As in the Neural Network example from Lecture 5 we will load the MNIST dataset, scale the inputs to between 0 and 1, and convert the Y labels to one-hot vectors. However unlike before we will not flatten the 28x28 image to a 784 element vector, since CNNs can inherently handle 2D data. We will use a batch size of 1 for now, you may want to try other values for your training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "# Load the data, and normalise it.\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5,), (0.5,))])\n",
    "# Change target to a one hot vector.\n",
    "target_transform = transforms.Compose(\n",
    "    [transforms.Lambda(lambda x: F.one_hot(torch.tensor(x), 10))])\n",
    "\n",
    "training_set = datasets.MNIST('data', train=True, download=True, transform=transform, target_transform=target_transform)\n",
    "test_set = datasets.MNIST('data', train=False, transform=transform, target_transform=target_transform)\n",
    "train_loader = torch.utils.data.DataLoader(training_set, batch_size = batch_size)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Create your CNN network\n",
    "\n",
    "Create your CNN network using pytorch (Hint: you may look at the CNN example from lecture 6). You should minimally have two convolutional layers, two maxpool layers, and at least one dense (linear) layer for the output. Use activation functions between layers such as `relu` or `softmax`. Pay careful attention to the size of the inputs and outputs. \n",
    "\n",
    "Write your class in the cell below and implement both the `constructor` and the `forward` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Enter your code for part 4.2 here in this code cell.\n",
    "\"\"\"\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        # call the parent constructor\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels = 1, out_channels = 32, kernel_size = (5,5))\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size = (2,2), stride = (2,2))\n",
    "        self.conv2 = nn.Conv2d(in_channels = 32, out_channels = 100, kernel_size = (5,5))\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size = (2,2), stride = (2,2))\n",
    "        self.fc1 = nn.Linear(in_features=100, out_features=10) # dense layer to do the classification\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.maxpool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.maxpool2(x)\n",
    "        output = self.fc1(x)\n",
    "        return output\n",
    "\n",
    "model = CNNModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Train your CNN network\n",
    "\n",
    "Now train your network on the training dataset. You should train for 50 to 100 epochs.\n",
    "\n",
    "You will have to decide on which optimizer to use (SGD, adam, for example), which loss function you will use, and any other hyperparameters such as learning rate, batch size, and momentum.\n",
    "\n",
    "Write your code to train in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (400x4 and 100x10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 30\u001b[0m\n\u001b[1;32m     28\u001b[0m (x, y) \u001b[38;5;241m=\u001b[39m (x\u001b[38;5;241m.\u001b[39mto(device), y\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mto(device))\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# perform a forward pass and calculate the training loss\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(pred, y)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# zero out the gradients, perform the backpropagation step,\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# and update the weights\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[24], line 21\u001b[0m, in \u001b[0;36mCNNModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     19\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(x)\n\u001b[1;32m     20\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaxpool2(x)\n\u001b[0;32m---> 21\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (400x4 and 100x10)"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Enter your code for part 4.3 here in this code cell.\n",
    "\"\"\"\n",
    "# define training hyperparameters\n",
    "lr = 1e-2\n",
    "num_epochs = 20\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "for e in range(0, num_epochs):\n",
    "\t# set the model in training mode\n",
    "\tmodel.train()\n",
    "\t# initialize the total training and validation loss\n",
    "\ttotalTrainLoss = 0\n",
    "\ttotalValLoss = 0\n",
    "\t# initialize the number of correct predictions in the training\n",
    "\t# and validation step\n",
    "\ttrainCorrect = 0\n",
    "\tvalCorrect = 0\n",
    "\t# loop over the training set\n",
    "\tfor i, (x, y) in enumerate(train_loader):\n",
    "\t\t# send the input to the device\n",
    "\t\t(x, y) = (x.to(device), y.to(torch.float32).to(device))\n",
    "\t\t# perform a forward pass and calculate the training loss\n",
    "\t\tpred = model(x)\n",
    "\t\tloss = criterion(pred, y)\n",
    "\t\t# zero out the gradients, perform the backpropagation step,\n",
    "\t\t# and update the weights\n",
    "\t\toptimizer.zero_grad()\n",
    "\t\tloss.backward()\n",
    "\t\toptimizer.step()\n",
    "\t\t# add the loss to the total training loss so far and\n",
    "\t\t# calculate the number of correct predictions\n",
    "\t\ttotalTrainLoss += loss\n",
    "\tprint(\"Epoch\", e, \"Training Loss:\", totalTrainLoss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Test your CNN network\n",
    "\n",
    "Now test your network on the test dataset. Print out the accuracy of your model. \n",
    "\n",
    "Try modifying your model and choosing different hyperparameters and see if you can improve the accuracy of your model. \n",
    "\n",
    "Write your code to test in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    Enter your code for part 4.4 here in this code cell.\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Enter your code for part 4.4 here in this code cell.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 5:\n",
    "\n",
    "Complete the following table with your final design (you may add more rows for the # neurons (layer1) etc. to detail how many neurons you have in each hidden layer). Likewise you may replace the lr, momentum etc rows with parameters more appropriate to the optimizer that you have chosen. (4 MARKS)\n",
    "\n",
    "| Hyperparameter       | What I used | Why?                  |\n",
    "|:---------------------|:------------|:----------------------|\n",
    "| Optimizer            |             |                       |\n",
    "| Input shape          |             |                       |\n",
    "| First layer          |             |                       |\n",
    "| Second layer         |             |                       |\n",
    "| Add more layers      |             |                       |\n",
    "| if needed            |             |                       |\n",
    "| Dense layer          |             |                       |\n",
    "| learning rate?       |             |                       |\n",
    "| momentum?            |             |                       |\n",
    "| loss function?       |             |                       |\n",
    "\n",
    "*FOR GRADER:* <br>\n",
    "*TABLE: _____ / 4* <br>\n",
    "*CODE: ______ / 10*<br>\n",
    "\n",
    "***TOTAL: ______ / 14 ***\n",
    "\n",
    "#### Question 6\n",
    "\n",
    "What is the final training and test accuracy that you obtained after 100 epochs. Are there signs of underfitting or overfitting? Explain your answer (4 MARKS)\n",
    "\n",
    "***Write your answer here***\n",
    "\n",
    "*FOR GRADER: ______ / 4*\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Conclusion\n",
    "\n",
    "In this lab we saw how to create a simple Dense neural network to complete the relatively simple task of learning how to classify irises according to their sepal and petal characteristics. We then tried using a CNN on the MNIST dataset. \n",
    "\n",
    "---\n",
    "\n",
    "***FOR TA ONLY***\n",
    "\n",
    "| Question |  Marks  |\n",
    "|:--------:|:-------:|\n",
    "|1         |     /4  |\n",
    "|2         |     /3  |\n",
    "|3         |     /5  |\n",
    "|4         |     /5  |\n",
    "|5         |     /14 |\n",
    "|6         |     /4  |\n",
    "|Total:    |     /35 |\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "055b62fcaec9821674a26809055da6bc29fe87d96b4c426e8bdfbe57b9f21334"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
