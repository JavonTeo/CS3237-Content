{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks\n",
    "\n",
    "Since our aim is to build machine learning models (whether statistical or neural network) to understand IoT data, let's begin by building some simple models in Python. \n",
    "\n",
    "In this lecture we look at implementing neural networks in Python. For the most part we will be using Pytorch, a high-level model-based library for implementing highly complex and sophisticated deep networks. \n",
    "\n",
    "But we will start small, and actually code a learning law on our own. We start by implementing a simple Single Layer Perceptron to learn the Lorry/Van Classification Problem that's in the lecture. The table below shows the data we have. We use -1 to mean \"Lorry\" and \"1\" to mean \"Van\":\n",
    "\n",
    "|Mass    |Length     |Class   |\n",
    "|:------:|:---------:|:------:|\n",
    "|10      |6          |-1      |\n",
    "|20      |5          |-1      |\n",
    "|5       |4          |1       |\n",
    "|2       |5          |1       |\n",
    "|3       |6          |-1      |\n",
    "|10      |7          |-1      |\n",
    "|5       |9          |-1      |\n",
    "|2       |5          |1       |\n",
    "|2.5     |5          |1       |\n",
    "|20      |5          |-1      |\n",
    "\n",
    "Let's begin by importing Numpy and creating our table by defining a function to create our datasets. Each input example contains the mass and length of the vehicle, and the labels are -1 for truck and 1 for van. The make_dataset function returns a 10x2 matrix for the input, and a 10x1 vector for the labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create our dataset\n",
    "def make_dataset():\n",
    "    train_data = np.array([[[10, 6]], [[20, 5]], [[5, 4]], \n",
    "                           [[2, 5]], [[3, 6]], [[10, 7]], \n",
    "                           [[5, 9]], [[2, 5]], [[2.5, 5]], \n",
    "                           [[20, 5]]])\n",
    "    train_labels = np.array([-1, -1, 1, 1, -1, -1, -1, 1, 1, -1])\n",
    "    \n",
    "    return (train_data, train_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now initialize the SLP. We define the SLP as a dictionary defined as follows:\n",
    "\n",
    "slp = {\n",
    "\n",
    "\"inputs\":<1x3 input vector>,\n",
    "\n",
    "\"weights\":<3x1 weights>,\n",
    "\n",
    "\"output\":<1x1 output>\n",
    "\n",
    "}\n",
    "\n",
    "Although we have only 2 inputs, our input is defined as 1x3 as we need to include the bias. There is only one output, and thus the weights will be 3x1 matrix of random numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the SLP:\n",
    "# We store our SLP as a dictionary. There are 3 inputs since we have Mass,\n",
    "# Length, and a bias which is always 1.0. There are 3 weights to connect\n",
    "# the 3 inputs to the output, and a single output\n",
    "\n",
    "def init_slp(slp):\n",
    "    slp['inputs'] = np.array([0.0, 0.0, 1.0])\n",
    "    slp['weights'] = np.random.randn(3, 1)\n",
    "    slp['output'] = np.array(0)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we come to the meat of the SLP: The feedforward and learn functions. The feedforward function is defined as:\n",
    "\n",
    "$$\n",
    "f(in, w) = \\tanh\\left(\\sum_{i=0}^{n-1}in_i \\times w_{i,0}\\right)\n",
    "$$\n",
    "\n",
    "Since we have defined our input as a $1\\times3$ matrix and the weights as a $3\\times1$ matrix, the feedforward is simply a matrix multiply.  We use a tanh transfer function since this maps us between -1 and 1. We set a parameter *alpha* to control the speed of learning. The learning function returns the absolute error, which we will later use to compute the mean absolute error (MAE) across all samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the feedfoward\n",
    "def feed_forward(slp, inputs):\n",
    "    # Take dot-product of the inputs and the weights\n",
    "    slp[\"inputs\"][0:2] = inputs\n",
    "    slp[\"output\"] = np.tanh(np.matmul(slp[\"inputs\"], slp[\"weights\"]))\n",
    "    return slp[\"output\"]\n",
    "\n",
    "def learn(slp, alpha, inputs, target):\n",
    "    feed_forward(slp, inputs)\n",
    "    \n",
    "    # Find error\n",
    "    E = target - slp['output']\n",
    "    slp[\"weights\"] = np.add(slp[\"weights\"], (alpha * E[0] \n",
    "                                          * slp['inputs'].reshape(3,1)))\n",
    "    return abs(E[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can create our SLP and train it. We iterate 600 times and print the MAE every 50 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Average Absolute Error: 0.99\n",
      "Iteration 50, Average Absolute Error: 0.46\n",
      "Iteration 100, Average Absolute Error: 0.44\n",
      "Iteration 150, Average Absolute Error: 0.43\n",
      "Iteration 200, Average Absolute Error: 0.43\n",
      "Iteration 250, Average Absolute Error: 0.42\n",
      "Iteration 300, Average Absolute Error: 0.42\n",
      "Iteration 350, Average Absolute Error: 0.42\n",
      "Iteration 400, Average Absolute Error: 0.41\n",
      "Iteration 450, Average Absolute Error: 0.35\n",
      "Iteration 500, Average Absolute Error: 0.01\n",
      "Iteration 550, Average Absolute Error: 0.01\n",
      "Iteration 600, Average Absolute Error: 0.01\n"
     ]
    }
   ],
   "source": [
    "slp = {}\n",
    "init_slp(slp)\n",
    "feed_forward(slp, np.array([[20.0, 5.0]]))\n",
    "\n",
    "(train_in, train_out) = make_dataset()\n",
    "for i in range(601):\n",
    "    ctr = 0\n",
    "    E = 0\n",
    "    for j, data in enumerate(train_in):\n",
    "        ctr = ctr + 1\n",
    "        E = E + learn(slp, 0.1, data, train_out[j])\n",
    "    \n",
    "    if (i % 50) == 0:\n",
    "        print(\"Iteration %d, Average Absolute Error: %3.2f\" % (i, E / ctr))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the MAE settles at a decent value of 0.01. Now we can try three sample inputs:\n",
    "\n",
    "|Mass    |Length     |\n",
    "|:------:|:---------:|\n",
    "|12      |7          |\n",
    "|3       |5          |\n",
    "|15      |12         |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mass\tLength\tClass\n",
      "-----\t------\t-----\n",
      "12.0\t7.0\ttruck\n",
      "3.0\t5.0\tvan\n",
      "15.0\t12.0\ttruck\n"
     ]
    }
   ],
   "source": [
    "test_inputs = np.matrix([[12, 7], [3, 5], [15, 12]])\n",
    "\n",
    "print(\"Mass\\tLength\\tClass\")\n",
    "print(\"-----\\t------\\t-----\")\n",
    "\n",
    "for x in test_inputs:\n",
    "    y = feed_forward(slp = slp, inputs = x)\n",
    "    veh_type = 'truck' if y<=0.0 else 'van'\n",
    "    print(\"%3.1f\\t%3.1f\\t%s\"% (x[0,0], x[0,1], veh_type))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we didn't put aside some of the training data for testing (there's only 10 of them), we don't have a \"gold standard\" to evaluate how good this SLP is. That's alright, since our main aim was to see how to implement the learning law. In any case the outputs here seem consistent with the training data (large mass, length -> truck, otherwise it's a van.)\n",
    "\n",
    "## Pytorch Models\n",
    "\n",
    "In this course we will use the Pytorch library to implement our neural networks. Pytorch is a convenient high-level library that is built on top of the Torch project, which is in turn a very large and complex library for machine learning.\n",
    "\n",
    "Full documentation on Pytorch can be found at [Pytorch docs](https://pytorch.org/docs/stable/index.html)\n",
    "\n",
    "Importantly, Pytorch is also definitely much more convenient to use than NumPy. ;)\n",
    "\n",
    "Let's begin with building a simple Multi-Layer Perceptron using the Sequential Model to recognize handwritten digits from the famous MNIST dataset.\n",
    "\n",
    "The MNIST dataset consists of a 28x28 black and white images of handwritten digits:\n",
    "\n",
    "![MNIST set](./Images/mnist.jpg)\n",
    "\n",
    "Our job then is to build a classifier that takes a 28x28 image and classify it as one of the 10 digits.\n",
    "\n",
    "We first will need to install torchvision, a python package with common datasets and models for computer vision tasks: `pip install torchvision`\n",
    "\n",
    "## Imports\n",
    "\n",
    "We begin by importing:\n",
    "\n",
    "    - torch.nn: The basic building blocks for our models.\n",
    "    - torch.nn.functional: The functions and other components for our models.\n",
    "    - torch.optim: The optimisers available in pytorch (such as SGD)\n",
    "    - torchvision: The python package with our MNIST dataset.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Designing and Building the Model\n",
    "\n",
    "Now let's begin building our model. The weights of the neural networks are called \"parameters\", and these are decided upon using an optimization algorithm. However we ourselves need to decide on \"hyperparameters\", which refer to the design of the NN:\n",
    "\n",
    "    - The size and shape of the input\n",
    "    - Encoding for the input\n",
    "    - # of hidden layers\n",
    "    - Size of each hidden layer\n",
    "    - Transfer functions\n",
    "    - Size of the output\n",
    "    \n",
    "Some of these are easy to determine. Since our inputs are 28x28 images and it's easier to work with a single dimension vector, we will reshape them into a single 784 element input. Hence the input layer will consist of 784 input nodes. We will scale all inputs to between 0 and 1 for performance reasons. There are 10 digits and one output node.\n",
    "\n",
    "For the rest we apply a combination of two well respected design techniques called \"intuition\" and \"guesswork\" and produce the following design:\n",
    "\n",
    "    - # of input nodes: 784\n",
    "    - Encoding: Scale between 0 and 1\n",
    "    - # of hidden layers: 2\n",
    "    - Size of hidden layer 1: 1024 nodes\n",
    "    - Transfer function: ReLU (see below)\n",
    "    - Size of hidden layer 2: 256 nodes\n",
    "    - Transfer function: ReLU\n",
    "    - Size of output: 1 node\n",
    "    \n",
    "The ReLU, Sigmoid (similar to Softmax) and other transfer functions are shown below. We saw these in the lecture:\n",
    "\n",
    "![Transfer Functions](./Images/transfer.png)\n",
    "\n",
    "We also add a \"dropout\" layer which randomly drops a percentage of the nodes for training, to reduce overfitting. We will look at this in more detail in a later lecture.\n",
    "\n",
    "Let's build our network. We first create a class `ModelNN` which inherits from `nn.Module` class and then add the layers in the constructor. We also specify the function `forward` where defines the connections between our layers in the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ModelNN, self).__init__()\n",
    "        # First hidden layer\n",
    "        self.l1 = nn.Linear(784, 1024)\n",
    "        # Randomly drop 30% of this layer for training\n",
    "        self.dropout1 = nn.Dropout(0.3)\n",
    "        # Add the second hidden layer. \n",
    "        self.l2 = nn.Linear(1024, 256)\n",
    "        # As before we randomly drop 30% of the nodes for training.\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "        self.l3 = nn.Linear(256, 11)\n",
    "    def forward(self, x):\n",
    "        # Here we define the forward function, we accept the input data \n",
    "        # and return the output data. We use modules defined in the constructor as\n",
    "        # well as any arbitrary operators.\n",
    "        # We can see this defines the connections between the layers of our model.\n",
    "        x = self.l1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.l2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        output = self.l3(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Dataset\n",
    "\n",
    "This is literally it! We've built the network! Now let's bring in the MNIST dataset. We will reshape the data from 28x28x1 to 784x1, load it with a `batch_size` of 60, and normalise the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████| 9912422/9912422 [00:00<00:00, 10800451.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/MNIST/raw/train-images-idx3-ubyte.gz to ../data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 28881/28881 [00:00<00:00, 24878967.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/MNIST/raw/train-labels-idx1-ubyte.gz to ../data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████| 1648877/1648877 [00:00<00:00, 10574757.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/MNIST/raw/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 4542/4542 [00:00<00:00, 4188770.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 60\n",
    "\n",
    "# Load the data, normalise it, and reshape it. \n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5,), (0.5,)),\n",
    "                                transforms.Lambda(lambda x: torch.flatten(x))])\n",
    "\n",
    "training_set = datasets.MNIST('../data', train=True, download=True, transform=transform)\n",
    "test_set = datasets.MNIST('../data', train=False, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(training_set, batch_size = batch_size)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Training\n",
    "\n",
    "Now that we have built the network, and loaded and properly encoded the data, let's start training. Here we will train the modeld. We will train for 10 epochs in batches of 60 samples. \"Batches\" are useful for controlling memory usage, especially when you are working in memory limited environments like GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [500/1000], Loss: 0.5517\n",
      "Epoch [1/10], Step [1000/1000], Loss: 0.1218\n",
      "Epoch [2/10], Step [500/1000], Loss: 0.2186\n",
      "Epoch [2/10], Step [1000/1000], Loss: 0.0603\n",
      "Epoch [3/10], Step [500/1000], Loss: 0.0886\n",
      "Epoch [3/10], Step [1000/1000], Loss: 0.0933\n",
      "Epoch [4/10], Step [500/1000], Loss: 0.1601\n",
      "Epoch [4/10], Step [1000/1000], Loss: 0.0375\n",
      "Epoch [5/10], Step [500/1000], Loss: 0.0819\n",
      "Epoch [5/10], Step [1000/1000], Loss: 0.1009\n",
      "Epoch [6/10], Step [500/1000], Loss: 0.1247\n",
      "Epoch [6/10], Step [1000/1000], Loss: 0.0968\n",
      "Epoch [7/10], Step [500/1000], Loss: 0.0861\n",
      "Epoch [7/10], Step [1000/1000], Loss: 0.0347\n",
      "Epoch [8/10], Step [500/1000], Loss: 0.0252\n",
      "Epoch [8/10], Step [1000/1000], Loss: 0.0148\n",
      "Epoch [9/10], Step [500/1000], Loss: 0.1010\n",
      "Epoch [9/10], Step [1000/1000], Loss: 0.0779\n",
      "Epoch [10/10], Step [500/1000], Loss: 0.0585\n",
      "Epoch [10/10], Step [1000/1000], Loss: 0.0552\n",
      "Training complete!\n",
      "Accuracy of the model on the 60000 training images: 98.55833333333334 %\n"
     ]
    }
   ],
   "source": [
    "# Model, Loss, and Optimizer\n",
    "model = ModelNN()\n",
    "\n",
    "# We use a \"cross entropy\" loss function which is more sophisticated \n",
    "# than the simple mean-squared loss function in the lecture and well \n",
    "# suited for classification problems.\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Create a Stochastic Gradient Descent optimizer with a learn rate of 0.01\n",
    "# and a momentum of 0.9 which helps control \"overshoot\"\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.01, momentum = 0.9)\n",
    "\n",
    "# Train the Model\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (data, labels) in enumerate(train_loader):\n",
    "        # Forward pass\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, labels)\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if (i+1) % 500 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
    "\n",
    "print(\"Training complete!\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in train_loader:\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f'Accuracy of the model on the 60000 training images: {100 * correct / total} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Finally once training is over we evaluate the network for performance:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on the 10000 test images: 97.77 %\n"
     ]
    }
   ],
   "source": [
    "# Test the Model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f'Accuracy of the model on the 10000 test images: {100 * correct / total} %')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
